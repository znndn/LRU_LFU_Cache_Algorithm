### **学习建议**

完成项目指导时间：7天完成（每天花6-8小时）  
首先大家需要先花两天时间学会LRU和LFU的基本思路并且完成相应代码，其次再去完成花三天时间学习并完成LRU和LFU的优化部分，最后再花两天时间学习并完成ARC部分代码。  
代码量：1100行左右  
基础要求：  
熟悉C++语法，学会C++11常用特性即可，了解操作系统相关知识，有网络编程经验更佳。

## **1.开篇**

开篇：帮助初次学习缓存项目的人能够更快的上手项目的开发  
项目名称：KamaCache  
项目地址：GitHub \- youngyangyang04/KamaCache: 【代码随想录知识星球】项目分享-cache的实现（LRU，LFU，ARC）  
此项目使用多个页面替换策略实现了一个线程安全的缓存系统：

* LRU：最近最久未使用页面置换算法  
* LFU：最不经常使用页面置换算法  
* ARC：自适应替换缓存算法

### **做完本项目，你的收获**

* 掌握缓存的作用、层次结构与设计哲学，理解其在系统架构中的关键价值  
* 亲手实现并优化LRU、LFU、ARC算法，应对不同业务场景  
* 运用互斥锁、原子操作及缓存分片技术，保证多线程数据一致性与高性能  
* 通过分片降低锁竞争、预加载预热缓存，显著提升吞htuns量与响应速度  
* 实践应对缓存穿透、击穿、雪崩的行业通用解决方案  
* 从数据结构设计、策略选择到性能评估，获得构建高性能中间件的完整经验

### **引言**

缓存策略在系统和实际业务开发过程中较为常见，下面我为大家列出一些使用了缓存策略的知名系统和组件：

#### **操作系统与存储系统**

**Linux Kernel：**

* **LRU:**  
  * 用于内存管理中页面缓存(Page Cache)，当物理内存不足时，系统会优先淘汰最近未使用的页面。  
  * 内核的虚拟文件系统(VFS)也采用LRU维护缓存的元数据

**Redis：**

* **LRU和LFU：**  
  * Redis 本身并不直接实现 LRU 缓存策略，但它提供了过期和淘汰机制。在内存不足时，Redis 可以配置不同的淘汰策略，如 volatile-lru（对于设置了过期时间的键，使用 LRU 策略淘汰数据）。  
  * Redis 提供了 LFU 策略（从 4.0 版本开始）作为其内存驱逐策略之一，可以通过 maxmemory-policy 配置。

**ZFS（Zettabyte File System）**

* **ARC:**  
  * ZFS文件系统中的缓存管理主要基于ARC  
  * ARC优于单纯的LRU或LFU，因为它能动态调整缓存策略以适应不同的访问模式

**PostgreSQL**

* **ARC:**  
  * 场景：在一些PostgreSQL的缓存拓展中实现了基于ARC的缓存机制

**Hadoop**

* **Hadoop(改进实现)**  
  * 场景：某些分布式存储的改进版本中，使用了类似ARC的动态缓存替换策略

#### **网络与负载均衡**

**NGINX**

* **LRU:**  
  * 用于缓存代理模式下的响应数据，淘汰最久未使用的缓存条目

#### **嵌入式系统与浏览器**

**Android内存缓存**

* **LRU:**  
  * Android的LruCache类用于管理有限内存中的应用数据缓存。

**Web浏览器**

* **LRU:**  
  * 浏览器的缓存机制（如历史记录、图片缓存）使用LRU策略来淘汰过期数据。

## **2.项目背景介绍**

### **什么是缓存？**

缓存是将高频访问的数据暂存到内存中，是加速数据访问的存储，降低延迟，提高吞吐率的利器。

### **为什么要实现缓存系统?**

因缓存的使用相关需求，通过牺牲一部分服务器内存，减少对磁盘或者数据库资源进行直接读写，可换取更快响应速度，尤其是处理高并发的场景，负责存储经常访问的数据，通过设计合理的缓存机制提高资源的访问效率。由于服务器的内存是有限的，我们不能把所有数据都存放在内存中，因此需要一种机制来决定当使用内存超过一定标准时，应该删除哪些数据，这就涉及到缓存淘汰策略的选择。

### **在什么地方加缓存？**

ps: 这里我们引用这篇文章中的解释：https://blog.csdn.net/chongfa2008/article/details/121956961  
缓存对于每个开发者来说是相当熟悉了，为了提高程序的性能我们会去加缓存，但是在什么地方加缓存，如何加缓存呢？  
举个例子：假设一个网站，需要提高性能，缓存可以放在浏览器，可以放在反向代理服务器，还可以放在应用程序进程内，同时可以放在分布式缓存系统中。  
从用户请求数据到数据返回，数据经过了浏览器、CDN、代理服务器、应用服务器，以及数据库各个环节。每个环节都可以运用缓存技术。从浏览器/客户端开始请求数据，通过HTTP配合CDN获取数据的变更情况，到达代理服务器(Nginx)可以通过反向代理获取静态资源。再往下来到应用服务器可以通过进程内（堆内）缓存，分布式缓存等方式获取数据。如果以上所有缓存都没有命中数据，才会回源到数据库。  
\*\*缓存的顺序：\*\*用户请求-\>HTTP缓存-\>CDN缓存-\>代理服务器缓存-\>进程内缓存-\>分布式缓存-\>数据库  
距离用户越近，缓存能够发挥的效果越好。

## **3.缓存淘汰策略**

### **缓存淘汰策略**

常见的有FIFO、LRU、LFU、ARC等，下面我将为大家详细介绍以上四种缓存淘汰策略。ps：前三种缓存淘汰策略的基础版本我会简略带过，因为网上的资料很多且详细，我重点讲一下LRU和LFU的待优化点，最后一种缓存淘汰策略ARC我会详细介绍一下。  
缓存污染：指访问很少的数据在服务完访问请求后还继续留在缓存中，造成缓存空间的浪费。  
缓存污染一旦变严重后，有大量不再访问的数据滞留在缓存中，往缓存中写入新数据时需要先把数据淘汰出缓存，引入额外的时间开销。

### **FIFO**

先进IENT算法，是最简单的页面替换策略，按照页面进入内存的顺序来决定替换哪个页面。即优先淘汰最早进入内存的页面，不论这些页面之后是否被频繁访问。但是FIFO可能会导致“Belady异常”，即随着分配给进程的物理块数增加，缺页次数反而增加。

### **LRU**

最近最少使用算法，如果一个数据最近被访问过，那么将来被访问的可能性也较大。因此，它选择最近最长时间未被访问的页面进行替换。LRU的性能和效率接近OPT，但是对于频繁访问的页面更新开销较大。  
潜在问题：淘汰热点数据，如果有个数据在1个小时的前59分钟访问了1万次(可见这是个热点数据),再后一分钟没有访问这个数据，但是有其他的数据访问，就导致了我们这个热点数据被淘汰。  
推荐一篇学习LRU算法的文章：https://blog.csdn.net/saxon\_li/article/details/123974696

#### **基础LRU代码**

学习完基础的LRU缓存算法实现之后，大家不难发现他身上的缺点：

* 对访问模式不敏感：如果是循环的一次性遍历大量不重复的数据（如A-\>B-\>C-\>D-\>A-\>B-\>...），LRU可能逐步清空，几乎无法命中。  
* 缓存污染：如果加载一些不再会被访问的冷数据（如一次性数据），将原有的热点数据挤出，冷数据留在缓存中，降低了缓存的利用率。  
* 不适用于某些场景：在某些场景下，最近最少使用并不代表最不重要或最少需要。  
* 锁的粒度大：多线程高并发的访问下，同步等待将是一笔极大的时间开销。

针对上述问题，可以本文进行了以下几点优化：

* LRU-k  
* HashLRU

#### **LRU-k**

参考：https://juejin.cn/post/6844904049263771662  
LRU-k算法是对LRU算法的改进，基础的LRU算法被访问数据进入缓存队列只需要访问(put、get)一次就行，但是现在需要被访问k（大小自定义）次才能被放入缓存中，基础的LRU算法可以看成是LRU-1。  
LRU-k算法有两个队列一个是缓存队列，一个是数据访问历史队列。当访问一个数据时，首先将其添加进入访问历史队列并进行累加访问次数，当该数据的访问次数超过k次后，才将数据缓存到缓存队列，从而避免缓存队列被冷数据所污染。同时访问历史队列中的数据也不是一直保留的，也是需要按照LRU的规则进行淘汰的。LRU-k执行过程如图：  
一般情况下，当k的值越大，缓存的命中率越高，但也使得缓存难以淘汰。综合来说，k \= 2 时性能最优。

#### **LRU-k 代码实现**

上述代码直接继承基础LRU。

#### **HashLRU**

参考：https://github.com/songangweb/mcache/tree/master  
有一定开发经验的同学一定发现了，该项目中锁的粒度很大，并且这个锁的粒度还不好直接减少毕竟涉及线程安全的地方实在是太多了，甚至连使用读写锁都没办法优化。那我们只好换个思路，将lru分片。 具体实现结构如图：  
根据传入的key值进行哈希运算后找到对应的lru分片，然后调用该分片相应的方法。  
大家试想一下当多个线程同时访问一个LRU时（LFU同理），由于锁的粒度大，是不是会造成长时间的同步等待，那如果是多个线程同时访问多个LRU缓存呢，是不是同步等待时间大大减少了。  
普通的lruCache和lfuCache在高并发情况下耗时增加的原因分析：  
线程安全的lfuCache中有锁的存在。每次读写操作之前都有加锁操作，完成读写操作之后还有解锁操作。在低QPS下，锁的竞争的耗时基本可以忽略；但在高并发的情况下，大量的时间消耗在等待锁的操作上，导致耗时增长。  
HashLruCache和HashLfuCache适应高并发场景：  
针对大量同步等待操作导致耗时增加的情况，解决方案就是尽量减小临界区。引入hash机制，对全量数据做分片处理，在原有LfuCache的基础上形成HashLfuCache，以降低查询耗时。  
HashLfuCache引入哈希算法，将缓存数据分散到N个LfuCache上，查询时也按照相同的哈希算法，先获取数据可能存在的分片，然后再去对应的分片上查询数据。这样可以增加lfu的读写操作的并行度，减少同步等待的耗时。

#### **HashLru代码实现：**

HashLruCaches和KLruCache类是组合关系，前者依赖于后者，KLruCache类的实现就在上文中。  
\*\*注意：\*\*LRU(LFU)分片之后淘汰的元素就不是全局中访问最近最不经常访问的元素了(全局的minFreq\_)，淘汰的就是局部minFreq\_，这里是可以接受的，因为可能某个分片LRU(LFU)中存着大量元素，下一个元素还分给这个LRU(LFU)分片的话，就应该将当前的LRU(LFU)中最近最不经常访问的元素剔除而不是全局的最近最不经常访问元素，然后将新元素添加进来。

### **LFU**

最近使用频率高的数据很大概率将会再次被使用,而最近使用频率低的数据,将来大概率不会再使用。  
做法：把使用频率最小的数据置换出去。这种算法更多是从使用频率的角度（但是当缓存满时，如果最低访问频次的缓存数据有多个，就需要考虑哪个元素最近最久未被访问）去考虑的。  
附上一篇学习LFU的好文：https://blog.csdn.net/saxon\_li/article/details/123985667  
学完基础的LFU，大家不难发现其身上还有很多待优化点：

* 频率爆炸问题：对于长期驻留在缓存中的热数据，频率计数可能会无限增长，占用额外的存储空间或导致计数溢出。  
* 过时热点数据占用缓存：一些数据可能已经不再是热点数据，但因访问频率过高，难以被替换。  
* 冷启动问题：刚加入缓存的项可能因为频率为1而很快被淘汰，即便这些项是近期访问的热门数据。  
* 不适合短期热点：LFU对长期热点数据表现较好，但对短期热点数据响应较慢，可能导致短期热点数据无法及时缓存。  
* 缺乏动态适应性：固定的LFU策略难以适应不同的应用场景或工作负载。  
* 锁的粒度大，多线程高并发访问下锁的同步等待时间过长。

针对上述问题，文本进行了以下几点优化：

* 加上最大平均访问次数限制  
* HashLFUCache

#### **最大平均访问次数限制**

参考：https://blog.csdn.net/yunshuipiao123/article/details/98958750  
在LFU算法之上，引入访问次数平均值概念，当平均值大于最大平均值限制时将所有结点的访问次数减去最大平均值限制的一半或者一个固定值。相当于热点数据“老化”了，这样可以避免频次计数溢出，也可以缓解缓存污染。

#### **加上最大平均访问次数限制的LFU代码**

设置最大平均访问次数的值解决了什么问题？

1. 防止某一个缓存的访问频次无限增加，而导致的计数溢出。  
2. 旧的热点缓存，也就是该数据之前的访问频次很高，但是现在不再被访问了，也能够保证他在每次访问缓存平均访问次数大于最大平均访问次数的时候减去一个固定的值，使这个过去的热点缓存的访问频次逐步降到最低，然后从内存中淘汰出去  
3. 一定程度上是对新加入进来的缓存，也就是访问频次为1的数据缓存进行了保护，因为长时间没被访问的旧的数据不再会长期占据缓存空间，访问频率会逐步被降为小于1最终淘汰。

#### **HashLfuCache**

实现类似于HashLruCache，理论部分直接看上文，代码实现如下：

### **ARC**

#### **核心思想：**

当访问的数据趋向于访问最近的内容，会更多地命中LRU list，这样会增大LRU的空间；当系统趋向于访问频繁访问的内容时，会更多地命中LFU list，这样会增加LFU的空间。

#### **应用场景：**

主要应用于不同的访问模式(比如近期频繁访问和周期性访问交叉的场景)之间表现较好，因为他可以动态调整缓存分区的大小。  
参考文章：

* [https://blog.csdn.net/liuyun2113/article/details/12705057](https://blog.csdn.net/liuyun2113/article/details/12705057)  
* [https://blog.csdn.net/sbsbsb666666/article/details/130198264](https://blog.csdn.net/sbsbsb666666/article/details/130198264)

下面引入一些其他地方对其的解释：ps: MFU代表LFU，MRU代表LRU  
相比于LFU和LRU其的优势在于：  
执行过程剖析：  
整个Cache分为两部分，LRU和LFU各占一半，根据不同的访问模式动态适应调整partition分割指针的位置（也就是动态调整lfu和lru的大小），除此之外，LRU和LFU各自有一个ghost list（淘汰链表，分别用于存储从LRU和LFU中淘汰的数据）。总体结构如图：  
更加详细结构流程图如下：（ps：ARCCache对象的get方法和put方法实际上调用的是LRU和LFU的get与put方法。 frequency指的是该数据的访问频率。）

1. 在缓存中查找客户端需要访问的数据，如果没有命中，表示缓存穿透，将需要访问的数据从从磁盘中取出，插入缓存中的LRU部分对应链表的尾部（ps：头插和尾插没有区别，看个人习惯）。  
2. 如果命中（LRU部分存在该数据）且LFU链表中没有则判断该数据的访问次数是否大于transformTime\_（自定义的变量，我这里设置为3，代表访问次数超过3次），若大于则将该数据插入LFU对应链表中（一个块至少需要读取transformTime\_次，并且要是最近请求的数据，才会被存储到LFU中）。于是，该数据块不仅仅只保存在LRU的缓存目录中，也将保存到LFU中。如果命中且LFU链表中存在，则将数据重新放入LFU链表中对应位置（访问频次计数+1），这样，那些真正被频繁访问的页面将一直呆在缓存中，不会被冷数据的加入而误淘汰，不经常访问的数据会向链表头部移动，最终被淘汰出去。  
3. 如果此时LRU缓存满了，则从LRU链表中淘汰表头部的数据，将淘汰数据的key放入LRU对应的ghost list。然后在LRU的链表尾部添加新数据。如果ghost list的元素满了，按照先进IENT的方式淘汰ghost list中的元素头部元素，然后再从尾部插入元素。  
4. 如未命中缓存的数据根据key发现在LRU对应的ghost list中，则表示幽灵命中，缓存系统就可以知道，这是一个刚刚淘汰的页面，而不是第一次读取或者说很久之前读取的一个页面。于是根据这个信息来调整内部的partition分割指针以适应当下的访问模式。上述迹象说明当前的LRU缓存太小了，于是将partition分割指针右移一位(也就是LRU缓存空间+1，LFU缓存空间-1），并将命中的key数据从ghost的中移除，将新数据从LRU链表尾部插入。  
5. 反之如果未命中缓存的数据根据key发现在LFU对应的ghost中，则说明当前访问模式偏向于频繁访问经常被访问的那一些数据，说明当下LFU缓存空间太小了，对应partition分割指针左移一位，并将命中的key数据从ghost的中移除，将新数据从LRU链表尾部插入。

#### **ARC代码实现**

该代码量较大，就不在文章中展示了，请移步：[https://github.com/youngyangyang04/KamaCache/blob/main/KArcCache.h](https://github.com/youngyangyang04/KamaCache/blob/main/KArcCache.h)

## **4.缓存策略对比**

### **缓存策略对比**

不管是使用LRU、LFU和ARC算法都是为了优化缓存命中率，确保最热数据优先留存在内存中，而将不常用或较久未使用的数据被淘汰。三种缓存策略各有优缺点，对比如下：

## **5.缓存命中率测试**

### **缓存测试结果展示**

该测试代码只是尽可能地模拟真实的访问场景，但是跟真实的场景仍存在一定差距，测试结果仅供参考。

### **缓存策略测试分析**

#### **1\. 测试场景概述**

##### **1.1 热点数据访问测试 (testHotDataAccess)**

**测试代码：**

**测试代码解释**

##### **测试特征：**

* 缓存容量：20  
* 操作次数：500,000  
* get访问缓存 和 put添加缓存比例，为了模拟真实场景，测试做了以下安排：  
  * 先进行预热，加载热点数据  
  * 测试过程中get和put随机交叉进行，put发生的概率是30%，get发生的概率是70%（反应了大多数缓存系统中读操作通常比写操作更频繁的特性）。  
* 实时记录了命中次数和总读取次数。  
* 数据访问偏向性模拟（模拟现实应用中的“二八法则”- 少量数据被大量访问，大量数据被少量访问）：  
  * 70% 访问热点数据（key范围：0-19）  
  * 30% 访问冷数据（key范围：20-4999）

##### **测试目的：**

* 验证缓存对频繁访问的少量数据的处理能力  
* 测试在大量冷数据干扰下的缓存性能

##### **有利策略：**

* LFU：因为能准确跟踪访问频率，对热点数据有很好的保留能力  
* ARC：能够通过自适应机制在列表中保留频繁访问的数据

##### **1.2 循环扫描测试 (testLoopPattern)**

**测试代码：**

**测试代码解释：**

##### **测试特征：**

* 缓存容量：30  
* 循环范围：500  
* 操作次数：80000  
* get访问缓存 和 put添加缓存比例，为了模拟真实场景，测试做了以下安排：  
  * 只预热了20%的数据，而不是全部500个键值对。这更符合真实场景，缓存通常是逐渐填充的，而不是一次性完全填满。  
  * 测试过程中get和put随机交叉进行，put发生的概率是20%，get发生的概率是80%（循环扫描场景读中，读操作更频繁）。  
* 实时记录了命中次数和总读取次数。  
* 访问模式：  
  * 60% 顺序扫描（每次 key \+ 1 % 循环范围）  
  * 30% 随机跳跃（循环范围内的随机数）  
  * 10% 范围外访问（循环范围外的随机数）

##### **测试目的：**

* 验证缓存在顺序访问模式下的表现  
* 测试缓存对周期性访问模式的适应能力  
* 评估缓存在数据局部性变化时的性能

##### **策略对比：**

* LRU的弱点：在循环扫描模式下，LRU容易发生"缓存抖动"，因为最近使用的数据可能立即被新的扫描数据替换。  
* LFU的表现：循环扫描模式下，如果某些数据被重复扫描多次，LFU可能会表现更好。  
* ARC的优势：ARC通过同时考虑访问频率和时间，可能在这种混合访问模式下有更平衡的表现。

##### **1.3 工作负载剧烈变化测试 (testWorkloadShift)**

**测试代码：**

**测试代码解释：**

##### **测试特征：**

* 缓存容量：30  
* 操作次数：80,000  
* get访问缓存 和 put添加缓存比例，为了模拟真实场景，测试做了以下安排：  
  * 读写交替进行，每个阶段的读写比例不同。  
  * 阶段1: 热点访问，15%写入。  
  * 阶段2: 大范围随机，写比例30%。  
  * 阶段3: 顺序扫描，10%写入。  
  * 阶段4: 局部性随机，微调为25%。  
  * 阶段5: 混合访问，调整为20%。  
* 五个不同访问阶段（每个阶段操作80,000 / 5 次）：  
  * 热点访问（少量key密集访问）  
  * 大范围随机访问（400个key随机访问）  
  * 顺序扫描（100个key，按顺序递增访问）  
  * 局部性随机访问（分区域随机）  
  * 混合访问模式（40%概率访问热点数据，30%概率随机访问中等范围数据，30%概率访问大范围数据）

##### **测试目的：**

* 评估缓存在工作负载突变时的适应能力  
* 测试不同缓存策略在复杂访问模式下的表现  
* 验证缓存算法的稳定性和自适应能力

##### **策略对比：**

* LRU可能在工作负载变化时迅速调整，但缺乏长期记忆  
* LFU可能保持长期记忆，但调整较慢  
* ARC应该能在这两者间取得平衡

#### **2\. 测试设计特点**

##### **2.1 数据操作分离**

* 将put和get操作分开执行  
* 避免了put后立即get导致的虚假高命中率

##### **2.2 动态数据范围**

* 通过动态改变数据访问范围  
* 模拟真实环境中数据分布的变化

##### **2.3 多样化访问模式**

* 结合顺序访问、随机访问和局部性访问  
* 更接近实际应用场景

#### **3\. 预期效果分析**

##### **3.1 LRU 策略**

* **优势场景：**  
  * 具有时间局部性的访问模式  
  * 工作集大小小于缓存容量的情况  
* **劣势场景：**  
  * 循环扫描大量数据  
  * 频繁访问的数据集大于缓存容量

##### **3.2 LFU 策略**

* **优势场景：**  
  * 稳定的热点数据访问  
  * 访问频率分布稳定的工作负载  
* **劣势场景：**  
  * 访问模式突变  
  * 临时突发的高频访问

##### **3.3 ARC 策略**

* **优势场景：**  
  * 复杂的混合访问模式  
  * 工作负载频繁变化  
  * 同时存在时间和频率局部性  
* **劣势场景：**  
  * 极端单一的访问模式（纯随机或纯顺序）

#### **4\. 测试结果分析要点**

* 关注命中率的绝对值  
* 观察不同策略在场景切换时的适应速度  
* 比较策略间的性能差异  
* 验证各策略在其优势场景中的表现

## **6.简历写法**

### **简历写法**

#### **一般简历写法**

##### **cpp-高并发缓存**

\*\*项目描述：\*\*基于C++实现的一个线程安全的高并发缓存系统，支持多种缓存替换策略（LRU、LFU、ARC）。项目开发注重系统的并发性能优化和缓存策略改进，以提高在高并发场景下的响应速度与命中率。

##### **主要工作**

* 实现多种缓存替换策略，适配不同的访问模式和业务场景；  
* 实现LRU和LFU的缓存分片，降低锁争用，提升高并发访问下的性能；  
* 实现LRU-k优化，防止热点数据被冷数据替换，减少缓存污染问题；  
* 实现LFU引入最大平均访问频次，淘汰旧的热点数据，提升缓存的整体利用率；  
* 实现ARC策略，动态调整LRU和LFU的权重比例，提升复杂环境下的缓存命中率；  
* 通过互斥锁和原子操作实现多线程下的线程安全；

#### **进阶简历写法**

##### **高并发缓存系统**

**项目描述：** 基于C++实现的线程安全的高并发缓存系统，支持多种缓存替换策略（LRU、LFU、ARC）。项目开发注重系统的并发性能优化和缓存策略改进，以提高在高并发场景下的响应速度与命中率。

##### **个人工作：**

* 实现了多种缓存替换策略，适配不同的访问模式和业务场景；  
* 实现了LRU和LFU的缓存分片，降低锁争用，提升高并发访问下的性能；  
* 实现了LRU-k优化，防止热点数据被冷数据替换，减少缓存污染问题；  
* 实现了LFU引入最大平均访问频次，淘汰旧的热点数据，提升缓存的整体利用率；  
* 实现了ARC策略，动态调整LRU和LFU的权重比例，提升复杂环境下的缓存命中率；  
* 通过互斥锁和原子操作实现多线程下的线程安全；

##### **项目难点：**

* 线程安全的实现，确保在高并发环境下数据的一致性和完整性；  
* 缓存分片策略的设计，以减少锁争用并提高并发性能；  
* LRU-k优化的实现，以解决热点数据被冷数据替换的问题；  
* LFU策略中最大平均访问频次的计算和应用，以提升缓存利用率；  
* ARC策略的动态权重调整算法，以适应不同业务场景下的缓存需求；

##### **个人收获：**

* 深入理解了多线程编程和线程同步机制，包括互斥锁和原子操作；  
* 掌握了缓存替换策略的设计与实现，包括LRU、LFU和ARC；  
* 提升了对高并发系统设计的理解，特别是在缓存系统中的应用；  
* 学习了如何通过性能分析和优化来提升系统在高并发场景下的表现；  
* 增强了对C++语言在系统级编程中的运用能力，特别是在内存管理和并发控制方面。

## **7.常见面试题**

这里的答案只是抛转引玉，答案大家可以根据自己的理解整理。  
部分题目参考：https://blog.csdn.net/qq\_40374604/article/details/128123061  
注意：部分相关解答可能跟项目本身有些不符合，欢迎在星球项目答疑群提出

### **你的缓存是什么？你的缓存机制是如何设计的？它用来缓存什么东西？具体在程序中用于哪些场景？**

答：缓存是将高频访问的数据暂存到内存中，是加速数据访问的存储，降低延迟，提高吞吐量。  
采用LRU，LFU等策略去实现缓存的。  
用于缓存一些频繁访问的数据，比如网页等文件资源、数据库的热点数据等。  
比如客户端向服务端请求某项文件资源时，服务端程序需要先将该文件资源从磁盘中读出到内存缓冲区中，然后再将其从内存缓冲区中发送给客户端。  
如果有了缓存机制，对于已经缓存了的文件，在客户端请求后，服务端就不用再向磁盘读取了，可以从缓存中直接取出文件传输给客户端。避免了读取磁盘带来的开销。高并发场景下频繁读取磁盘也会造成较大的性能开销。

### **你用缓存机制缓存了数据库数据吗？**

缓存是可以用于缓存数据库的数据的，因为数据库访问（特别是复杂查询）通常非常慢  
数据变更频率较低，但访问频率高  
如果数据是“热点数据”，就更适合缓存

### **什么是缓存？**

答：缓存，就是数据交换的缓冲区，是一种用于临时存储数据的高效存储机制，其主要目的是加快访问速度、减轻后台系统压力，从而提升整体性能。我们平时说的缓存大多是指内存。目的是，把读写速度慢的介质的数据保存在读写速度快的介质中（这里的快与慢是相对概念），从而提高读写速度，减少时间消耗。例如：  
CPU高速缓存：告诉缓存的读写速度远高于内存。  
CPU读数据时，如果在高速缓存中找到所需数据，就不需要读内存  
CPU写数据时，先写到高速缓存，再写回内存。  
磁盘缓存：磁盘缓存其实就把常用的磁盘数据保存在内存中，内存读写速度也是远高于磁盘的。  
读数据时从内存中读取  
写数据时，可先写回内存，定时或定量写回到磁盘，或者是同步写回。

### **请说说有哪些缓存算法？是否能手写一下LRU代码的实现？**

缓存算法中，比较常见的如下：

* FIFO(先进先出)  
* LRU(最近最少使用)  
* LFU(最不经常使用)  
* ARC(自适应替换)

LRU代码实现如下：

### **常见的缓存工具和框架有哪些？**

在C++开发中，常见的缓存工具和框架如下：

* Boost.Cache：提供灵活的缓存管理，支持LRU和LFU等策略。  
* Redis：支持多种数据结构的分布式缓存。也是最主流最常用的。  
* Intel TBB Cache：提供线程安全的缓存实现，支持并发访问。

### **为什么需要实现这个项目？使用缓存的目标是什么？**

思路：阐述高并发场景下缓存的意义，例如提升性能、降低数据库压力、优化用户体验等。

### **多线程下如何保证线程安全？你使用了什么技术或方法来实现线程安全？**

思路：

* 使用 std::mutex、std::shared\_mutex 等同步机制避免数据竞争。  
* 如果有分片优化，可以提到分片缓存降低锁粒度，减少锁竞争。  
* 使用原子操作（如 std::atomic）优化性能。

### **实现了哪些缓存策略？如何选择？各自的优缺点是什么？**

思路：

* 说明实现了 LRU、LFU、ARC 策略及其优化：  
* LRU 分片：减少并发访问的锁冲突。  
* LFU 分片：降低访问频率更新的竞争。  
* ARC：动态平衡命中率和访问频率的需求。  
* 对比这些策略的应用场景和缺点。

### **如何存储缓存数据？数据结构是如何设计的？**

思路：

* 提到使用 std::unordered\_map（哈希表）作为快速查找的基础。  
* 使用双向链表（如 std::list）维护访问顺序，实现 LRU 或 LFU 的高效淘汰。

### **每种缓存策略的时间复杂度和空间复杂度分别是多少？**

思路：

* LRU：O(1) 查找、O(1) 插入/删除。  
* LFU：O(log 1\) 更新频率。  
* ARC：动态调整需要额外的复杂性分析。

### **你对 LRU 和 LFU 策略做了哪些优化？如何验证这些优化的效果？**

思路：

* LRU 分片：减少锁冲突。  
* LRU-k：解决热点数据被冷数据挤出问题。  
* LFU 增加最大平均访问频率：避免历史热点长期占用缓存。

### **多线程高并发下，如何优化缓存的性能？**

思路：

* 缓存分片：每个分片使用单独的锁，降低锁粒度。  
* 读写分离：大量读操作可使用 std::shared\_mutex 提升性能。  
* 非阻塞操作：可以提到 CAS（Compare-And-Swap）优化。

### **如何评估缓存的性能？测试指标有哪些？**

思路：

* 测试指标包括命中率、吞吐量、延迟。  
* 通过压测工具（如 Webbench、ab）模拟高并发场景，验证性能是否达标。

### **问题：如何选择锁机制？为什么要降低锁的粒度？**

思路：

* 锁的粒度越小，锁冲突越低，但管理复杂度增加。  
* 使用读写锁时，读多写少场景下可以显著提高性能。

### **如何同时支持 LRU、LFU 和 ARC 策略？用户如何选择不同的策略？**

思路：

* 提供抽象基类或接口，设计多种策略的实现。  
* 用户可以通过配置或运行时参数选择不同的策略。

### **如果需要新增一种缓存策略，如何设计？**

思路：

* 基于面向对象编程的思想，新增策略只需继承接口并实现。

### **如何应对缓存穿透、缓存击穿和缓存雪崩问题？**

思路：

* 缓存穿透：使用布隆过滤器过滤无效查询。  
* 缓存击穿：对热点数据加锁，避免缓存失效后高并发访问后端存储。  
* 缓存雪崩：错开缓存过期时间，或设置合理的回源策略。

### **Redis有哪些淘汰策略**

Redis的淘汰策略有以下几种：

* LRU算法：当内存不足以容纳新写入数据时，移除缓存中最近最少访问的数据  
* LFU算法：当内存不足以容纳新写入数据时，移除缓存中最不经常访问的数据  
* FIFO算法：最早放入缓存的数据最先被删除  
* Random算法：随机移除某个键

当涉及到设置了过期时间的数据时，还有以下策略：

* volatile-lru：从设置了过期时间的数据中选择最近最少使用的数据淘汰。  
* volatile-lfu：从设置了过期时间的数据中选择最不经常访问的数据淘汰。  
* volatile-random：从设置了过期时间的数据中随机选择数据淘汰。  
* volatile-lfu：从设置了过期时间的数据中选择过期时间最近的键淘汰。

### **什么是缓存污染，你是如何减少这个问题的？**

思路：

* 缓存污染定义：指不重要的数据占据了缓存空间，从而挤出更重要的、经常访问的数据。  
* 解决措施：  
* 使用 LRU-k：缓存只淘汰短期未访问的数据，防止热点数据被冷门数据挤出。  
* 增加缓存淘汰策略的灵活性，例如结合访问频率和时间综合判断。

### **你是如何实现缓存分片的？**

思路：

* 将缓存划分成多个片段，每个片段有独立的数据结构和锁。  
* 根据键值的 哈希值 确定对应的分片，从而减少锁争用。  
* 每个分片独立管理内存空间和替换策略，以便最大限度优化并发性能。

### **在C++中，原子操作是如何实现线程安全的？**

思路：

* 使用 std::atomic 提供的操作，例如 std::atomic\<int\>，避免因竞态条件导致数据不一致。  
* 底层实现：通过硬件提供的 原子指令（如 CPU 的 CAS）来确保操作的不可分割性。  
* 优势：无需加锁，减少线程之间的阻塞，提高性能。

### **你是如何动态调整ARC策略中的权重比例的？**

思路：

* 根据实时监控的命中率（命中频率 vs. 未命中频率）进行调整。  
* 逻辑实现：  
* LRU 部分权重：针对短期热点数据。  
* LFU 部分权重：针对长期热点数据。  
* 动态调整：命中率下降时，增大 LFU 权重（防止频繁访问的数据被替换）；反之增大 LRU 权重。  
* 示例：实现一个自适应机制，通过调节阈值动态切换缓存分区的占比。

### **在设计高并发缓存系统时，你考虑了哪些性能指标？**

思路：

* 响应速度：缓存访问的延迟应尽可能低（如纳秒级）。  
* 命中率：缓存被访问时，命中次数占总访问次数的比例。  
* 吞吐量：系统每秒可以处理的请求数量。  
* 资源利用率：内存占用、CPU 消耗，以及网络 I/O 等的效率。  
* 扩展性：是否可以轻松扩展缓存容量或支持更高并发量。

### **你是如何优化缓存系统以提高高并发场景下的响应速度的？**

思路：

* 使用 缓存分片，减少全局锁的使用。  
* 优化替换策略，减少复杂度。  
* 利用 本地性原则（如将常用缓存分配在线程本地存储中）。  
* 调整数据结构，例如使用 哈希链表 或 跳表 以减少查询时间复杂度。

### **你是如何确保在高并发环境下数据的一致性的？**

思路：

* 使用 细粒度锁：只锁定需要操作的部分数据，而不是全局锁定。  
* 使用原子操作确保对共享数据的访问安全。

### **在C++中，你是如何管理内存的？**

思路：

* 使用 智能指针：std::shared\_ptr 和 std::unique\_ptr 自动管理对象生命周期，避免内存泄漏。  
* 使用 RAII 原则：在构造时分配资源，析构时自动释放资源。  
* 定期清理：缓存中淘汰的对象及时清除，以释放内存。  
* 预分配内存池：减少频繁的内存分配操作，优化性能。

### **你是如何实现缓存的预热功能的？**

思路：

* 预加载策略：在系统启动时，分析历史数据，加载常用或热点数据到缓存中。  
* 实现方式：提供接口将静态数据加载到缓存。例如：

1  
2  
复制代码  
cache.put("key", "value"); // 手动加载

* 优势：减少启动后首次访问的延迟，提高用户体验。

### **在实现缓存分片时，你是如何确定分片数量的？**

思路：

* 根据系统的 CPU 核心数，设置分片数量与核心数一致，利用多核性能。  
* 根据预期的 并发量 和 负载，通过压力测试确定分片数量，以确保负载均衡。  
* 分片数与哈希函数结合，避免数据分布不均。

### **你是如何确保缓存系统在面对突发流量时的稳定性的？**

思路：

* 负载均衡：通过多个缓存节点分担请求压力。  
* 熔断机制：当缓存压力过大时，临时降级，直接返回默认值或空值。  
* 提前扩容：在高并发场景下，动态扩展缓存容量或缓存节点。